const jsonData = [
    {
        "id": 1,
        "path": "partiful",
        "image": "partiful",
        "title": "PARTIFUL",
        "deployed": "https://www.figma.com/file/SMauVPWF602N5hPJN2X7n6/Partiful%253A-Past-Events?type=design&node-id=1%3A2&mode=design&t=V4fVaZhx1FABGJgi-1&fuid=1006571621293291392",
        "duration": "a 3 week project", 
        "time": "April 2024",
        "team": "Ivery Chen, Dave Song, Sohum Sanu, Viviana Wei",
        "role": "UIUX Design",
        "tools": "Figma, Loom",
        "eventName": "Partiful Events Page Redesign",
        "description": "Redesigned the Partiful Events Home Page.",
        "categories": ["Prototyping", "UIUX Design", "Figma"],
        "section1": [
            {
                "type": "section-text",
                "content": "Context",
            },
            {
              "type": "subheading",
              "content": "How do you redesign the landing page so that more people would navigate to ‘Past Events’?"
            },
            {
              "type": "text",
              "content": "Our team worked with Google Ventures Backed startup, Partiful, to redesign their events page. Partiful is a website and app that allows users to create delightful event pages for birthdays, hangouts, and everything in between. Hosts can invite friends and friends-of-friends when they don't have a phone number or socials. Event pages build hype around the party, allowing guests and hosts to interact with each other."
            },
            {
              "type": "subheading",
              "content": "What are some of the existing problems in current design?"
            },
            {
              "type": "image",
              "content": "partiful_problem",
              "image_descript": "Original screens"
            }
          ],
        "section2": [
            {
              "type": "section-text",
              "content": "Research",
            },
            {
              "type": "subheading",
              "content": "What are our success metrics?"
            },
            {
              "type": "list",
              "content_list_1": "A More users navigating to past events",
              "content_list_2": "B More activity (comments, comment replies, photos) on past events",
              "content_list_3": "C Users connecting with one another from past events"
            },
            {
              "type": "subheading",
              "content": "Who are our customers?"
            },
            {
              "type": "text",
                "content": "Our customers are Gen Z + Millennials, hyper-social and always wanting to meet and connect with new friends through campus club events, birthdays and other greek events. We want to explore design language that is familiar to GenZ. Some inspiration include Instagram Feed, iPhone notifications and Snapchat SnapMaps."
            },
            {
                "type": "subheading",
                "content": "Who did we talk to?"
              },
              {
                "type": "text",
                  "content": "We interviewed Partiful superhosts to ask about their experiences using Partiful, and whether or not they spend much time at all on the app. One of the good insight we gained is that users don’t tend to use Partiful to look at past memories at events, instead, they want to discover new ones in a more effective manner."
              }
        ],
        "section3": [
            {
              "type": "section-text",
              "content": "Process",
            },
            {
              "type": "image-text-v-container",
              "content_stack_1": "Individually, we sketched out our own versions of user flow based on client expectations and problems.",
              "content_stack_2": "We met with founders along each step of the way, modifying lo-fi prototypes as we go.",
              "image_stack_v_1": "partiful_sketch1",
              "image_stack_v_2": "partiful_sketch2",
              "descript_image_stack_v": "Lo-fi sketches",
            },
            {
              "type": "image-stack-h-container",
              "image_stack_h_1": "partiful_wireframe1",
              "image_stack_h_2": "partiful_wireframe2",
              "image_stack_h_3": "partiful_wireframe3",
              "image_stack_h_4": "partiful_wireframe4",
              "image_stack_h_5": "partiful_wireframe5",
              "num_images": 5,
              "descript_image_stack_h": "Lo-fi prototype",
            }
        ],
        "section4": [
            {
              "type": "section-text",
              "content": "Solution",
            },
            {
              "type": "subheading",
              "content": "How did we respond to founder feedback?"
            },
            {
              "type": "text",
              "content": "The founders’ feedback were that we should focus on the landing page instead of extra functionalities. We decided to create three distinct versions of the page based on our inspirations, Instagram feed, iPhone notification and Snapchat Snapmap, to showcase a range of unique ideas."
            },
            {
              "type": "image-text-h-container-left",
              "content_stack_1": "Past events are too hidden, the app doesn’t feel like a social media platform. Many users don’t have the app downloaded",
              "content_stack_2": "To make past events accessible from the landing page, implementing infinite scroll to encourage time-spent in past events and connecting with other users",
              "image_stack_h_1": "partiful_basic_hifi",
              "image_stack_h_2": "partiful_basic_hifi2",
              "num_images": 2,
              "descript_image_stack_h": "Basic design",
            },
            {
              "type": "image-text-h-container-left",
              "content_stack_1": "The app doesn’t have a ‘viral factor’ that incentivises users to open and browse through the app",
              "content_stack_2": "To make Partiful feel more like a social media platform, we created a feed version that aims to encourage users to shift their focus from events alone to the attendees",
              "image_stack_h_1": "partiful_notif_past",
              "image_stack_h_2": "partiful_notif_expanded_past",
              "num_images": 2,
              "descript_image_stack_h": "Feed-based design",
            },
            {
              "type": "image-text-h-container-left",
              "content_stack_1": "Interviewees reflect that the app has to offer more than event management. There has to be something more motivating for users to download the app",
              "content_stack_2": "To add to the fun brand image of Partiful, a map similar to Snapchat’s Snap-map is shown here. One can see past events and where they are located geographically. Animation can indicate whether a party is happening soon, and eventually a heat-map can be used to help hosts and users alike visualise trend and event patterns. This solution was the founders’ favourite as it makes Partiful more powerful of a party management platform that offers data analytics",
              "image_stack_h_1": "partiful_map_design",
              "image_stack_h_2": "partiful_map_design_past",
              "num_images": 2,
              "descript_image_stack_h": "Map-based design",
            },
        ]
    },
    {
        "id": 2,
        "path": "foam",
        "image": "foam",
        "title": "FOAM",
        "deployed": "https://foam-phi.vercel.app/",
        "duration": "a 24 hr sprint project", 
        "time": "April 2024",
        "team": "Ivery Chen",
        "role": "UIUX Design, Frontend Engineering",
        "tools": "Figma, React, HTML, Javascript, CSS",
        "eventName": "Foam Web App",
        "description": "Designed the Pinterest for local artists discovery.",
        "categories": ["Frontend", "Prototyping", "UIUX Design", "Figma", "React", "Javascript", "HTML", "CSS"],
        "section1": [
          {
              "type": "section-text",
              "content": "Context",
          },
          {
            "type": "subheading",
            "content": "How do you design a site for local-artist gig discovery?"
          },
          {
            "type": "text",
            "content": "FOAM is a Pinterest-like portfolio site for art students, a local-artist discovery site for clients to keep track of their favourite local artists. As a RISD student, I constantly get asked, “Do you know a Graphic designer for a concert campaign?” The interface is similar to Pinterest, but it differs in that you can get in touch with real artists behind the work. The site would allow for filtering, aggregating and sorting. "
          }
        ],
        "section2": [
          {
              "type": "section-text",
              "content": "Research",
          },
          {
            "type": "subheading",
            "content": "What are some of the things I like from other sites that have these functionality?"
          },
          {
            "type": "text",
            "content": "FOAM is a Pinterest-like portfolio site for art students, a local-artist discovery site for clients to keep track of their favourite local artists. As a RISD student, I constantly get asked, “Do you know a Graphic designer for a concert campaign?” The interface is similar to Pinterest, but it differs in that you can get in touch with real artists behind the work. The site would allow for filtering, aggregating and sorting. "
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "foam_compare",
            "num_images": 1,
            "descript_image_stack_h": "Comparison Table between features",
          }
        ],
        "section3": [
          {
            "type": "section-text",
            "content": "Process",
          },
          {
            "type": "image-text-v-container",
            "content_stack_1": "Next, I created mockups for the Landing and Favourites page in Figma to help with the design process.",
            "image_stack_v_1": "foam_figma_favourite",
            "image_stack_v_2": "foam_figma_landing_page",
            "descript_image_stack_v": "Figma prototype",
          },
        ],
        "section4": [
          {
              "type": "section-text",
              "content": "Solution",
          },
          {
            "type": "text",
            "content": "I implemented all the target filter, aggregation and sort functions. All filters appear as drop down menus. I also created a 'select all' checkbox for most menus for easy selection of multiple categories. "
          },
          {
            "type": "text",
            "content": "As an additional fun thing, I created a notification popup animation when a liked item is added. It also indicates which image you liked! "
          },
          {
            "type": "text",
            "content": "One surprising challenge is displaying image as landscape or horizontal, and making sure all empty gaps are filled in. To do this, I used column-gap instead of flexbox."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "foam_landing",
            "num_images": 1,
            "descript_image_stack_h": "Final implemented website",
          }
        ],
    },
    {
        "id": 3,
        "path": "sf",
        "image": "sf",
        "title": "FUNCHEAPSF",
        "deployed": "https://iverychen.github.io/FunCheapSFRedesign/",
        "duration": "a 2 week project", 
        "time": "March 2024",
        "team": "Ivery Chen",
        "role": "UIUX Design, Frontend Engineering",
        "tools": "Figma, HTML, Javascript, CSS",
        "eventName": "FunCheapSF Redesign",
        "description": "Redesigning funcheapsf.com.",
        "categories": [ "Frontend", "Prototyping", "UIUX Design", "Figma", "Javascript", "HTML", "CSS"],
        "section1": [
          {
              "type": "section-text",
              "content": "Context",
          },
          {
            "type": "subheading",
            "content": "How do you redesign and improve a website, such as funcheapsf.com, and make it responsive?"
          },
          {
            "type": "text",
            "content": "FunCheapSF is your ultimate guide to budget-friendly entertainment and activities in San Francisco. I spent my  two summers in the Bay Area as a student and used the website frequently to find cheap activities to do. One of the things I realized is that there are many problems with the site that makes it difficult to navigate and find the information you need."
          },
          {
            "type": "subheading",
            "content": "What are some of the existing problems in current design?"
          },
          {
            "type": "image",
            "content": "sf_problem",
            "image_descript": "Original screen"
          }
        ],
        "section2": [
          {
            "type": "section-text",
            "content": "Research",
          },
          {
            "type": "subheading",
            "content": "What are my success metrics?"
          },
          {
            "type": "list",
            "content_list_1": "A Reduce visual clutter and repetition",
            "content_list_2": "B Implement filter functionality for better search experience",
            "content_list_3": "C Reorganise object hierarchy",
            "content_list_4": "D Design uniform event template",
            "content_list_5": "E Design for responsive screens, for laptop, iPad and iPhone",
          },
          {
            "type": "subheading",
            "content": "What sites did I reference?"
          },
          {
            "type": "text",
            "content": "In general, I looked at websites that also allow for search for events, such as TripAdvisor. I also looked at websites that had a lot of categories of things and learned from how they organised filters around that."
          }
        ],
        "section3": [
          {
            "type": "section-text",
            "content": "Process",
          },
          {
            "type": "image-text-v-container",
            "content_stack_1": "I did a speed sketch  to get some rough ideas on a page.",
            "content_stack_2": "Based on the quick sketch exercises, one design/layout emerged. It contains a logo bar, a filter bar with categories, and a section for event cards to be displayed.",
            "image_stack_v_1": "sf_speedy1",
            "image_stack_v_2": "sf_speedy2",
            "descript_image_stack_v": "Pen and paper sketches",
          },
          {
            "type": "image-stack-v-container",
            "image_stack_v_1": "sf_maclow",
            "image_stack_v_2": "sf_ipadlow",
            "image_stack_v_3": "sf_phonelow",
            "num_images": 3,
            "descript_image_stack_v": "Lo-fi prototype",
          }
        ],  
        "section4": [
          {
              "type": "section-text",
              "content": "Solution",
          },
          {
            "type": "subheading",
            "content": "What colour palette should I choose?"
          },
          {
            "type": "text",
            "content": "For the style guide, I chose minimal colours as otherwise it was too distracting, and I decided to use yellow and dark navy blue to indicate time of day in the timeline design."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "sf_styleguide",
            "num_images": 1,
            "descript_image_stack_h": "Styleguide",
          },
          {
            "type": "image-text-v-container",
            "content_stack_1": "Reduce visual clutter and repetition with a side bar for main headings and separate out newsletter and city guide to its own page",
            "content_stack_2": "Implement filter functionality for better search experience",
            "image_stack_v_1": "sf_phonehigh2",
            "image_stack_v_2": "sf_ipadhigh2",
            "image_stack_v_3": "sf_machigh2",
            "descript_image_stack_v": "Final Designs",
          },
        ],
    },
    {
        "id": 4,
        "path": "drivealive",
        "image": "drivealive",
        "title": "DRIVEALIVE",
        "deployed": "",
        "duration": "a 3 week project", 
        "time": "May 2024",
        "team": " Ivery Chen, Dave Song, Hadley Dalton, Healey Koch",
        "role": "UIUX Design, Frontend Engineering",
        "tools": "Figma, Python, Javascript",
        "eventName": "DriveAlive",
        "description": "A Computer Vision project that detects driver drowsiness.",
        "categories": [ "Frontend", "Prototyping", "UIUX Design", "Figma", "Computer Vision", "Python"],
        "section1": [
          {
              "type": "section-text",
              "content": "Context",
          },
          {
            "type": "subheading",
            "content": "How to design an interface that detects drowsy driving?"
          },
          {
            "type": "list",
            "content_list_1": "Drowsy driving ranks among the top causes of traffic accidents, significantly affecting road safety. Providing early warnings to sleepy drivers could prevent numerous accidents on the roads. ",
            "content_list_2": "According to data collected by the U.S. Department of Transportation over the past decade, car accidents, including both injuries and fatalities, are increasing. It is imperative that we use our abilities as software engineers to promote safer driving. ",
            "content_list_3": "For our purpose, we want to create a site that allows one to upload/stream a video and it suggests whether someone is drowsy driving or not. "
          }
        ],
        "section2": [
          {
              "type": "section-text",
              "content": "Research",
          },
          {
            "type": "subheading",
            "content": "How do we determine what makes someone ‘drowsy’?"
          },
          {
            "type": "text",
            "content": "We determine how many drowsy blinks one exhibits in a short, seconds-long clip. First, the system identifies the face area in every frame of the video within each second. Then, it pinpoints the eye area using a facial landmarks detector. Next, it calculates and analyzes the eye aspect ratio for each frame. Afterward, three types of classifiers—linear SVM and sequential neural network—are used to enhance accuracy. The data is then classified to determine whether the driver's eyes are open or closed. The system detects closed eyes for a set period within each second, tallying the amount of drowsy blinks persisting within or throughout the seconds. The amount of drowsy blinks per minute is then quantified. More than even one drowsy blink should urge you to reconsider driving."
          }
        ],
        "section3": [
          {
            "type": "section-text",
            "content": "Process",
          },
          {
            "type": "list",
            "content_list_1": "1. Trained a CNN on images of open and closed eyes",
            "content_list_2": "2. Used Haarcascades to locate the face and isolate the eyes, which were then sent to the CNN for classification",
            "content_list_3": "3. If the eyes were closed for 15 frames (or half a second) it was counted as a “drowsy” blink "
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "drivealive_archi",
            "num_images": 1,
            "descript_image_stack_h": "Model Architecture",
          }
        ],
        "section4": [
          {
              "type": "section-text",
              "content": "Solution",
          },
          {
            "type": "text",
            "content": "We created a web page with UI to what the real product would look like."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "drivealive_interface",
            "num_images": 1,
            "descript_image_stack_h": "Model Architecture",
          }
        ]
    },
    {
        "id": 5,
        "path": "bayer",
        "image": "bayer",
        "title": "BAYER",
        "deployed": "",
        "duration": "a 3 month internship", 
        "time": "May 2022",
        "team": " Ivery Chen, Joshua Robinson (Tech support), Brenda Teakert (PM), Jason Sankovitch (Manager)",
        "role": "Unity Engineer, Technical Artist, Programmer, VR UX Design",
        "tools": "Unity, Substance Painter, AWS, Oculus Desktop, Blender",
        "eventName": "Bayer VR",
        "description": "An end-to-end Unity training app to each drone flyers how to operate drones.",
        "categories": [ "Unity", "AR/VR/MR", "UX Research", "Blender", "Substance Painter", "AWS", "Oculus", "C#"],
        "section1": [
          {
              "type": "section-text",
              "content": "Context",
          },
          {
            "type": "subheading",
            "content": "How do you design a VR educational app to train drone flyers how to assemble drones? "
          },
          {
            "type": "text",
            "content": "Bayer’s Crop Design division put in a request - many UAV flyers felt like they needed a tool to help them train new flyers that would reduce time spent for trainers travelling to and fro. They wanted a VR training app that can take care of a large part of the training. When I was onboarded, the app was merely an idea. My role was to implement from start to finish an entire VR app. The project aims to design a cost-effective, flexible and sustainable module for UAV flyers."
          }
        ],
        "section2": [
          {
              "type": "section-text",
              "content": "Research",
          },
          {
            "type": "subheading",
            "content" : "What do the customers really need?",
          },
          {
            "type": "text",
            "content": "To decide what goes into the module, I conducted research with the UAV flyers at Bayer. Some of the highlights are:"
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "bayer_research",
            "num_images": 1,
            "descript_image_stack_h": "Interview questions and results",
          }
        ],
        "section3": [
          {
            "type": "section-text",
            "content": "Process",
          },
          {
            "type": "text",
            "content": "Next, I retopologised the M300 model inside Blender, reduced total drone poly count from 600,000 to 20,000. I also remodelled parts so drone can be assembled, twisted, folded in VR. I brought everything into Substance Painter and created realistic materials ready to import into Unity."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "bayer_process",
            "num_images": 1,
            "descript_image_stack_h": "Modelled, retopologised and shaded assets",
          },
          {
            "type": "subheading",
            "content": "How does the experience unfold?",
          },
          {
            "type": "text",
            "content": "Before I wrote any code, I brought everything into Unity. I tested out the models to see if they are ‘functional’, foldable and constructable in VR. I then created a storyboard to plan out the beat of the experience."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "bayer_storyboard",
            "num_images": 1,
            "descript_image_stack_h": "Interview questions and results",
          }
        ],
        "section4": [
          {
              "type": "section-text",
              "content": "Solution",
          },
          {
            "type": "subheading",
            "content": "What are the challenges of UX design in VR?",
          },
          {
            "type": "text",
            "content": "The experience is controlled by a tablet. This is a continuation of design made by Tristan Lassiter, my hiring manager, on previous VR projects."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "bayer_ux",
            "num_images": 1,
            "descript_image_stack_h": "UX design using colour and highlight",
          },
          {
            "type": "text",
            "content": "Since the headsets we are using do not support HandTracking 2.0, I came up with alternative ways to imitate small motions with clunky hand controllers."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "bayer_uxchart",
            "num_images": 1,
            "descript_image_stack_h": "Three ways to suggest twist movement",
          }
        ]
    },
    {
        "id": 6,
        "path": "panda",
        "image": "panda",
        "title": "PANDA",
        "deployed": "",
        "duration": "a 3 weeks project", 
        "time": "Nov 2021",
        "team": "Ivery Chen (UIUX, Frontend engineering, Front/backend intergration), Zigzag (UIUX, Frontend engineering), Mazine Sulima (Backend), Oren Kohavi (Backend, Networking), Helen huang (Rules page, Producer), Omari Charles (Algorithm for allowed steps)",
        "role": "UIUX Design",
        "tools": "Figma, React, Javascript, CSS",
        "eventName": "Panda",
        "description": "Kahoot-style multiplayer game.",
        "categories": [ "Frontend", "Backend", "Prototyping", "UIUX Design", "React", "Javascript", "HTML", "CSS"],
        "section1": [
          {
              "type": "section-text",
              "content": "Context",
          },
          {
            "type": "subheading",
            "content": "Digitising a popular cardgame, in the style of Kahoot"
          },
          {
            "type": "text",
            "content": "A popular card game in Asian called Panda, but on the web! Multiplayer mode enabled to allow 4 players at a time. Real time update of changes and result."
          }
        ],
        "section2": [
          {
            "type": "section-text",
            "content": "Research",
          },
          {
            "type": "text",
            "content": "I worked on front-end back-end integration (Javascript, CSS), front-end functionality such as selecting cards and highlighting, styling, FIGMA prototype, and real time update of react components."
          },
          {
            "type": "text",
            "content": "This game requires good integration between front-end and back-end that can render and update the page instantly for smooth gameplay; cards, the winner of the round are displayed dynamically. Back-end sends gameStatus to the front-end every second, but frequent update erases effects such as highlighting, a technical challenge I overcame."
          },
          {
            "type": "text",
            "content": "I learnt a lot about React and how to smoothly display real-time changes on the front-end."
          }
        ],
        "section3": [
          {
            "type": "section-text",
            "content": "Process",
          },
          {
            "type": "text",
            "content": "Lo-fi UI design"
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "panda_ui",
            "num_images": 1,
            "descript_image_stack_h": "Figma prototype",
          }
        ],
        "section4": [
          {
            "type": "section-text",
            "content": "Process",
          },
          {
            "type": "video",
            "content": "panda_video",
            "source": "vimeo"
          }
        ]
    },
    {
        "id": 7,
        "path": "skim",
        "image": "skim",
        "title": "SKIM",
        "deployed": "",
        "duration": "a 30 hr sprint project", 
        "time": "April 2024",
        "team": "Ivery Chen",
        "role": "Game Design",
        "tools": "Unity, Photoshop",
        "eventName": "Skim",
        "description": "A Haiku-from-novel game",
        "categories": [ "Unity", "Game Design", "C#", "MQTT"],
        "section1": [
          {
            "type": "section-text",
            "content": "Context",
          },
          {
            "type": "subheading",
            "content": "What if you can write Haikus out of any existing novel?"
          },
          {
            "type": "text",
            "content": "In total I spent 30 hours on this game sprint. I will be changing the game controls a little to improve on usability soon."
          },
          {
            "type": "subheading",
            "content": "Learning"
          },
          {
            "type": "list",
            "content_list_1": "Creating a scoring system for Haiku.",
            "content_list_2": "Taking text and auto-formatting into unity objects that look like words on a page"
          },
          {
            "type": "video",
            "content": "skim_video",
            "source": "youtube"
          }
        ],

    },
    {
        "id": 8,
        "path": "magritte",
        "image": "magritte",
        "title": "MAGRITTE VR",
        "deployed": "",
        "duration": "a 2 week project", 
        "time": "May 2022",
        "team": "Ivery Chen (Texturing, modelling, programming), Sarah (Texturing, modelling), Katrina (Research and essay), Abhinav (Programming), Aidan (Sound Design)",
        "role": "ARVR Unity Programmer, Tech Artist",
        "tools": "Unity, Blender, Oculus Desktop, Photoshop",
        "eventName": "Magritte VR",
        "description": "If Magritte had access to VR technology, what would his paintings be like?",
        "categories": [ "Unity", "AR/VR/MR", "Blender", "C#"],
        "section1": [
          {
              "type": "section-text",
              "content": "Context",
          },
          {
            "type": "subheading",
            "content": "Thinking about simulation and VR-like experiences throughout history"
          },
          {
            "type": "text",
            "content": "This is a 3 week in-class group project on the topic of “Simulating Reality” at Brown University. I pitched the idea and took lead in modelling, texturing, setting up the project, programming and integrating VR controllers with our game mechanics."
          },
          {
            "type": "text",
            "content": "A 3D rendition of Magritte’s Painting, La Vengeance, in VR. There is nothing more challenging than recreating the works of Surrealist painters in 3D, as surrealism is meant to depict illogical, unrealistic scenes."
          }
        ],
        "section2": [
          {
              "type": "section-text",
              "content": "Research",
          },
          {
            "type": "subheading",
            "content" : "What would Magritte do if he had access to VR?",
          },
          {
            "type": "text",
            "content": "Magritte questions the status of representation in his paintings. This is a VR experience that allows users to experience both possible realities of his intentions for this painting."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "magritte_painting",
            "num_images": 1,
            "descript_image_stack_h": "René Magritte’s La Vengeance",
          },
          {
            "type": "text",
            "content": "The viewer will be able to walk around the room and enter the painting within the painting, a second ‘reality’. The user is able to switch between a landscape painting + cloud wallpaper to real mountains and real clouds."
          }
        ],
        "section3": [
          {
            "type": "section-text",
            "content": "Process",
          },
          {
            "type": "text",
            "content": "Modelling is straight forward, the most challenging part is the window shader."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "magritte_clouds",
            "image_stack_h_2": "magritte_mountains",
            "num_images": 2,
            "descript_image_stack_h": "Assets created by Sarah Jihyun Woo",
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "magritte_blender",
            "num_images": 1,
            "descript_image_stack_h": "Room + Painting + Interior (assets created by myself)",
          },
          {
            "type": "subheading",
            "content": "How to create a portal shader?",
          },
          {
            "type": "text",
            "content": "I created these paint textures in Photoshop and Blender to mimic the texture of the original painting. Some of the textures were done using Image Projection (such as the metal ball)."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "magritte_texture",
            "num_images": 1,
            "descript_image_stack_h": "Painting textures I created",
          },
          {
            "type": "text",
            "content": "One of the biggest challenges was creating the shader for the 3D scene inside the painting, so that only the parts contained inside the painting is shown, and the rest hidden."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "magritte_shader_ref",
            "num_images": 1,
            "descript_image_stack_h": "Image from Alastair Aitchison",
          },
          {
            "type": "text",
            "content": "We found existing resource online on Stencil Buffer, adapted them and wrote our own shaders that preserves the texture of the objects but also allow them to “disappear” outside of certain boundaries. We figured our variations of the script for the easle, clouds, mountains, painting frame, which were all using slightly different combinations of code."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "magritte_shader",
            "num_images": 1,
            "descript_image_stack_h": "Shaders we created with scripts",
          },
          {
            "type": "video",
            "source": "vimeo",
            "content": "magritte_video"
          },
          {
            "type": "subheading",
            "content": "What were some of the challenges?",
          },
          {
            "type": "text",
            "content": "We initially had access only to an HTC Vive, but after setting our project up, we realised that there were many limitations."
          },
          {
            "type": "subheading",
            "content": "Space Limitation",
          },
          {
            "type": "list",
            "content_list_1": "The roof in the model is low and painting appears small because the physical room is small",
            "content_list_2": "Limited to this space because we are using the Vive headset plugged into a PC at Fulvio’s lab",
            "content_list_3": "We are able to touch two physical walls but not the other two",
            "content_list_4": "We could possibly set dress the space so people can feel more with their hands while exploring the space - prop walls, easel and canvas? does that take away or add something?"
          },
          {
            "type": "subheading",
            "content": "Programming Limitation",
          },
          {
            "type": "list",
            "content_list_1": "After setting up, we realised that it is very difficult to get the users’ eye-line to line up with the exact point we would want them to for a seamless transition between the 3D clouds (real) and projected clouds (painting)",
            "content_list_2": "We decided to use a button that allows users to click to switch between the 2 forms",
            "content_list_3": "Alternatively, we could teleport people back to the exact point and remove the walking component, but we think what makes this project more immersive is the ability to walk around the room."
          },
          {
            "type": "subheading",
            "content": "Shadows",
          },
          {
            "type": "list",
            "content_list_1": "It looked strange that when we look down there is no body or shadow.",
          },
          {
            "type": "text",
            "content": "We eventually got access to an Oculus Quest 2 and used it for our project.",
          }
        ],
        "section4": [
          {
              "type": "section-text",
              "content": "Solution",
          },
          {
            "type": "text",
            "content": "Below are side-by-side comparison images of the projected 2D world and the 3D world. The switch is triggered by pressing the buttons on the sides of both controllers to switch back and forth. The switch is also accompanied by music that switches on and off."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "magritte_demo1",
            "num_images": 1,
            "descript_image_stack_h": "Left: projected clouds on wall, Right: 3D clouds",
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "magritte_demo2",
            "num_images": 1,
            "descript_image_stack_h": "Left: painting of environment, Right: a portal into environment",
          }
        ]
    },
    {
        "id": 9,
        "path": "nasa_suits",
        "image": "nasa_suits",
        "title": "NASA SUITS",
        "deployed": "https://github.com/ViolaTan55/RISD-SUITS2022/tree/main",
        "duration": "a 4 month project", 
        "time": "Jan 2022",
        "team": "Ivery Chen (Navigation), Kienan (Tech lead, Figma bridge, GPS telecommunication, deployment), Viola (Tech lead, Interaction, deployment), Ben M (Figma bridge) ",
        "role": "MR Programmer",
        "tools": "Unity, MRTK, Figma",
        "eventName": "NASA SUITS",
        "description": "Mixed Reality Hololens app for NASA Suits Challenge.",
        "categories": [ "Unity", "AR/VR/MR", "Blender", "Hololens", "C#"],
        "section1": [
          {
              "type": "section-text",
              "content": "Context",
          },
          {
            "type": "subheading",
            "content": "How do we design and implement a Mixed Reality app on Hololens for Space Exploration?"
          },
          {
            "type": "text",
            "content": "RISD DesignAR’s user interface design focuses on simple and clear visuals that allow for quick access to the main functions astronauts need to perform while in space. The user interface allow the user to checkout the map for navigation, gives out warning messages for communication, allows users to get their vital information."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "suits_design",
            "image_stack_h_2": "suits_design_map",
            "num_images": 2,
            "descript_image_stack_h": "Left: Figma mock up, Right: Navigation mock up design",
          },
          {
            "type": "text",
            "content": "A 3D rendition of Magritte’s Painting, La Vengeance, in VR. There is nothing more challenging than recreating the works of Surrealist painters in 3D, as surrealism is meant to depict illogical, unrealistic scenes."
          }
        ],
        "section2": [
          {
              "type": "section-text",
              "content": "Research",
          },
          {
            "type": "subheading",
            "content": "How do we create real-time navigation without any data?"
          },
          {
            "type": "text",
            "content": "For this project, I am on the programming team, in charge of the Navigation component. This means that the app has to work at the NASA test-site in Houston under dim lighting, where the 'astronaut' can select two locations on the map and be able to follow the arrows to arrive there. Since NASA didn't provide us with any real data to work with, I invented solution to the challenge of AR‐GPS navigation by integrating real life GPS data with Unity’s built‐in NavMesh Agent."
          }
        ],
        "section3": [
          {
              "type": "section-text",
              "content": "Process",
          },
          {
            "type": "text",
            "content": "I hand-painted a height map and created a terrain that the Nav Mesh agent uses to navigate from point A to point B. Inside the interface, it looks like a straight line that rests on the environment ahead of us. My teammates Kienan and Viola integrated GPS data with my scripts."
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "suits_unity",
            "num_images": 1,
            "descript_image_stack_h": "Using Unity NavMesh Agent",
          }
        ],
        "section4": [
          {
              "type": "section-text",
              "content": "Solution",
          },
          {
            "type": "text",
            "content": "Recovered footage from the HoloLens of NASA evaluator testing our SUITS’ team’s navigation portion of the software."
          },
          {
            "type": "subheading",
            "content": "What are the biggest challenges?"
          },
          {
            "type": "text",
            "content": "Initially, I thought that I would have to write my own path-finding algorithm but after knowing that we have no data about the test-site but a bird-view, and that we cannot use Azure for locations, I realised that we could use Unity's built-in navigation algorithm by using NavMesh agent."
          },
          {
            "type": "text",
            "content": "Another huge challenge is getting Unity and MRTK set up on my MacBook. Setting up DuoBoot and windows 10 on my computer took way longer (and crashed way more frequently) than I had expected it to."
          },
          {
            "type": "text",
            "content": "Recovered footage from the HoloLens of NASA evaluator testing our SUITS’ team’s navigation portion of the software."
          },
          {
            "type": "subheading",
            "content": "Accomplishments I'm proud of"
          },
          {
            "type": "text",
            "content": "On the final test date, my teammates who were able to make it to the site were able to integrate my Navigation project into the main project, and were able to hook up the GPS data with my code and navigation between sites!"
          },
          {
            "type": "subheading",
            "content": "What I learned"
          },
          {
            "type": "text",
            "content": "There aren't always existing solutions out there, sometimes it takes a very long time to do research, to try out methods, but the trial and error is always worth it and is part of learning!"
          },
        ]
    },
    {
        "id": 10,
        "path": "threedeepro",
        "image": "threedeepro",
        "title": "THREEDEEPRO",
        "deployed": "",
        "duration": "a 6 months project", 
        "time": "Nov 2023",
        "team": "Ivery Chen, Ji Won Chung (mentor)",
        "role": "Software development",
        "tools": "Python, Websockets, OpenCV, Javascript",
        "eventName": "Threedeepro",
        "description": "A computer vision, on-paper drawing based 3D modelling tool",
        "categories": [ "Backend", "Prototyping", "Research", "Computer Vision", "Computer Graphics", "Python", "Javascript"],
        "section1": [
          {
              "type": "section-text",
              "content": "Context",
          },
          {
            "type": "subheading",
            "content": "How do we design a 3D prototyping tool with only pen, paper and laptop camera?"
          },
          {
            "type": "text",
            "content": "A tool that generates 3D assets with paper using OpenCV, ThreeJS, MediaPipe, Websockets, earcut algorithms, and writing custom obj files. It brings back traditional/gestural methods with drawings, hand gestures and laptop webcam into creating 3d assets within seconds."
          },
          {
            "type": "text",
            "content": "Developed Holofilter in Javascript, a web interaction system that uses screen gyroscopes to enhance user engagement with a mixture of 2D SVGs and 3D assets integrating Three.js."
          }
        ]
    },
    {
        "id": 11,
        "path": "crystal",
        "image": "crystal2",
        "title": "CRYSTAL GEN",
        "deployed": "",
        "duration": "a 3 weeks project", 
        "time": "Nov 2022",
        "team": "Ivery Chen (Obj file, Seed generation), Nicolas Vadasz (Mesh generation), Healey Koch (Seed generation), Hadley Dalton (UI)",
        "role": "Graphics Programmer",
        "tools": "OpenGL, C++, QtCreator",
        "eventName": "Procedurally generate crystals",
        "description": "A tool that generates crystals.",
        "categories": [ "Backend", "Computer Graphics", "C++", "QTCreator"],
        "section1": [
          {
              "type": "section-text",
              "content": "Context",
          },
          {
              "type": "video",
              "content": "crystal_video",
              "source": "youtube"
          }
        ]
    },
    {
        "id": 12,
        "path": "usd",
        "image": "usd",
        "title": "USD MATERIAL TOOL",
        "deployed": "",
        "duration": "a weekend project", 
        "time": "May 2024",
        "team": "Ivery Chen",
        "role": "Software Developer",
        "tools": "Python, Houdini, USD",
        "eventName": "Texture to Material.usd tool",
        "description": "A tool that takes in models and textures and auto-create usd material files.",
        "categories": [ "Backend", "Prototyping", "Computer Graphics", "USD", "Python"],
        "section1": [
          {
            "type": "section-text",
            "content": "Context",
          },
          {
            "type": "subheading",
            "content": "Creating a tool that creates USD materials and binds to models"
          },
          {
            "type": "text",
            "content": "This project converts meshes and texture files in an `Assets` directory and creates a folder named `materials` inside with materials created from the input. The script prints out a visual representation of a dependency graph and reports any missing meshes or textures."
          },
          {
            "type": "text",
            "content": ""
          },
          {
            "type": "text",
            "content": "Developed Holofilter in Javascript, a web interaction system that uses screen gyroscopes to enhance user engagement with a mixture of 2D SVGs and 3D assets integrating Three.js."
          }
        ],
        "section2": [
          {
            "type": "section-text",
            "content": "Research",
          },
          {
          "type": "subheading",
          "content": "Learning USD system",
          },
          {
            "type": "text",
            "content": "The biggest challenge for me is that I haven't previously worked with USD files before. After failing to install OpenUSD, I decided to swtich to Houdini Karma which has a tool similar to OpenUSD. Using that I was able to debug my code. I also looked into USD documentation to learn how to bind materials."
          }
          ],
        "section3": [
          {
            "type": "section-text",
            "content": "Process",
          },
          {
            "type": "subheading",
            "content": "The Approach"
          },
          {
            "type": "list",
            "content_list_1": "The code creates a copy of the input usd file, creates a material from the texture files and bind the material to the output mesh in the usd. For this project, I chose to output a .usd file. ",
            "content_list_2": "I created a class called `Material`, a class called `Mesh`, and a dictionary called `meshes` that has key value pairs of name -> Mesh.",
            "content_list_3": "The script accounts for missing textures and meshes by reporting it in the terminal.",
            "content_list_4": "The main call to `create_materials()` is at the bottom of the file. `create_materials()` goes through every file in the input directory, `extract_info` to properly extract the file_name and file_types of the files, populates the `meshes` dictionary, and goes through all items in `meshes` to `create_use_material()`. It also makes a call to `output_dependency_graph()`, which prints out the dependency graph in the terminal with error messages for missing textures or meshes.",
            "content_list_5": "Inside `create_usd_material()`, `copy_usd_file()` is called to make a duplicate of the input usd file, create Shaders and bind to the output usd file that is located in ./Assets/materials.",
            "content_list_6": "`check_materials()` can be called to check if a material is correctly bounded to a mesh."
          }
        ],
        "section4": [
          {
            "type": "section-text",
            "content": "Solution",
          },
          {
            "type": "subheading",
            "content": "What were some of the assumptions?"
          },
          {
            "type": "text",
            "content": "Inside `create_usd_material`, assumption is made that for the MetallicRoughnessTexture, two separate inputs of `metallic` and `roughness` are created. I dug around online to find some information about which channel they each are and decided to gow ith `b` and `g` for metallic and roughness respectively."
          },
          {
            "type": "subheading",
            "content": "Outcome"
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "usd_output4",
            "image_stack_h_2": "usd_output5",
            "num_images": 2,
            // "descript_image_stack_h": "Example output USD files with binded materials",
          },
          {
            "type": "image-stack-h-container",
            "image_stack_h_1": "usd_output3",
            "image_stack_h_2": "usd_output1",
            "image_stack_h_3": "usd_output2",
            "num_images": 3,
            "descript_image_stack_h": "Example output USD files with binded materials",
          }
        ]
    },
    {
        "id": 13,
        "path": "reel",
        "image": "reel",
        "title": "TECH ART REEL",
        "deployed": "",
        "duration": "a 3 year ongoing project", 
        "time": "May 2024",
        "team": "Ivery Chen",
        "role": "Tech Artist, Animator, Director",
        "tools": "Blender, Maya, Houdini, Substance Painter, Nuke, Unity, Katana, Premiere Pro, After Effects",
        "eventName": "Tech Artist Reel",
        "description": "Tech Art Reel",
        "categories": ["Blender", "Maya","Houdini", "Substance Painter", "Nuke", "Unity", "Katana", "Premiere Pro", "After Effects"],
        "section1": [
          {
            "type":"section-text",
            "content":"Context"
          },
          {
            "type":"video",
            "source": "vimeo",
            "content":"reel_video"
          }
        ]
    },
    {
      "id": 14,
      "path": "ascension",
      "image": "ascension",
      "title": "Ascension",
      "deployed": "https://brownrisdgames.itch.io/ascension",
      "duration": "a 4 month project", 
      "time": "Jan 2022",
      "team": "Ivery Chen (3D Environment Artist), Nicole Strubinski (Producer, 3D Artist), Evan Samuel Mickelson (Programmer, Level Designer, and Producer), Ayman Benjelloun Touimi (Programmer), Seong-Heon Jung (Programmer), Dexter McChesney (Level and Gameplay Designer), Healey Koch (3D Animator and Artist), Catherine Hackl (3D Character Artist), Zhiying Shi (3D Asset Artist), Melody Yu (UI/UX Designer, 3D Environmental Design), Ashley Chung (UI/UX Designer), Zackary Entwistle (Sound Designer), Christine Baek (Sound Designer), David Jung (Sound Designer)",
      "role": "3D Environment Artist",
      "tools": "Blender, Unity",
      "eventName": "Ascension",
      "description": "A first person shooting game in zero gravity",
      "categories": ["Unity", "Game Design", "Blender", "C#"],
      "section1":[
        {
          "type":"section-text",
          "content":"Context"
        },
        {
          "type":"text",
          "content":"Play as a cute space explorer in a zero-gravity space station! Use your ray gun to ascend through the twists and turns and knock evil aliens out of your way!          "
        },
        {
          "type": "text",
          "content":"I created versatile assets, including 6 different tiles to be used to set dress out first person shooting game scene. I also created other assets such as tables and assembled one out of the three scenes using Unity."
        }
      ],
      "section2": [
        {
          "type":"section-text",
          "content":"Research"
        },
        {
          "type":"text",
          "content":"Below are Miro board inspirations we've gathered as a team as we design the game together."
        },
        {
          "type": "image-stack-v-container",
          "image_stack_v_1": "ascension_moodboard1",
          "image_stack_v_2": "ascension_moodboard2",
          "num_images": 2,
          "descript_image_stack_v": "Miro Board Concepts",
        }
      ],
      "section3": [
        {
          "type":"section-text",
          "content":"Process"
        },
        {
          "type": "image-stack-h-container",
          "image_stack_h_1": "ascension_asset1",
          "image_stack_h_2": "ascension_asset2",
          "image_stack_h_3": "ascension_asset3",
          "num_images": 3,
          "descript_image_stack_h": "Modular assets in progess",
        },
        {
          "type": "image-stack-h-container",
          "image_stack_h_1": "ascension_demo1",
          "num_images": 1,
          "descript_image_stack_h": "Ascension gameplay to show assembled environment",
        }
      ],
      "section4": [
        {
          "type":"section-text",
          "content":"Solution"
        },
        {
          "type":"video",
          "source": "youtube",
          "content":"ascension_video"
        }
      ]
  },
  {
    "id": 15,
    "path": "ballmaze",
    "image": "ballmaze",
    "title": "Ballmaze",
    "deployed": "https://simmer.io/@IveryChen/ballmaze",
    "duration": "a 1 month project", 
    "time": "Nov 2021",
    "team": "Ivery Chen",
    "role": "3D Asset Creation, Game Design, Game Programming, Level Design",
    "tools": "Blender, Unity",
    "eventName": "ballmaze",
    "description": "A Unity 3D game for the web",
    "categories": ["Unity", "Game Design", "Blender", "C#"],
    "section1":[
      {
        "type":"section-text",
        "content":"Context"
      },
      {
        "type":"text",
        "content":"BallMaze is a solo original game inspired by the traditional ball-in-a-maze game, except you are inside! By moving around, you can tilt the maze to get the ball to the Bunny! Modelled, animated, scripted in Blender and Unity. "
      }
    ],
    "section2":[
      {
        "type":"section-text",
        "content":"Research"
      },
      {
        "type":"text",
        "content":"As my first Unity project, I watched numerous Youtube tutorials. I learned how to script for Unity, use Animation controller, Animate in Blender for game purposes, design levels and creating assets for splash screen as well as creating UI that reflects current gamestate."
      }
    ],
    "section3":[
      {
        "type":"section-text",
        "content":"Process"
      },
      {
        "type":"text",
        "content":"I modelled, scripted the game. I wanted it to be slightly frustrating - as the traditional ball-in-a-maze game is. I learnt Unity for this project.        "
      },
      {
        "type":"subheading",
        "content":"Things I've learned",
      },
      {
        "type":"list",
        "content_list_1": "Model swapping - changing the model when the bot is killed",
        "content_list_2": "Fake gravity/weight script",
        "content_list_3": "Randomly spawn lightning",
        "content_list_4": "Particle effects for robot death",
        "content_list_5": "Unity UI/Canvas"
      }
    ],
    "section4":[
      {
        "type":"section-text",
        "content":"Solution"
      },
      {
        "type":"video",
        "source":"vimeo",
        "content":"ballmaze_video"
      }
    ]
  }

];

export default jsonData;
